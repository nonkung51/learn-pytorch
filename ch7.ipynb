{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ch7.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1__uG2NC2FD3JNNNDcU7-WcuJVMCsK-dJ","authorship_tag":"ABX9TyMSDqqaGYwoKxj8Jala6HFq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Ponpd72dfPGj","colab_type":"code","outputId":"f8931f1b-0623-4089-96a9-8a5853cc2291","executionInfo":{"status":"ok","timestamp":1584369981000,"user_tz":-420,"elapsed":429,"user":{"displayName":"Nonthakon Jitchiranant","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQRf1pDpuAGCoa63WoiIEb1FlqGrP4SE6snYtKLA=s64","userId":"05164243114410212485"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["import os\n","import cv2\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","REBUILD_DATA = False # set to true to one once, then back to false unless you want to change something in your training data.\n","\n","class DogsVSCats():\n","    IMG_SIZE = 50\n","    CATS = \"PetImages/Cat\"\n","    DOGS = \"PetImages/Dog\"\n","    TESTING = \"PetImages/Testing\"\n","    LABELS = {CATS: 0, DOGS: 1}\n","    training_data = []\n","\n","    catcount = 0\n","    dogcount = 0\n","\n","    def make_training_data(self):\n","        for label in self.LABELS:\n","            print(label)\n","            for f in tqdm(os.listdir(label)):\n","                if \"jpg\" in f:\n","                    try:\n","                        path = os.path.join(label, f)\n","                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n","                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n","                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  # do something like print(np.eye(2)[1]), just makes one_hot \n","                        #print(np.eye(2)[self.LABELS[label]])\n","\n","                        if label == self.CATS:\n","                            self.catcount += 1\n","                        elif label == self.DOGS:\n","                            self.dogcount += 1\n","\n","                    except Exception as e:\n","                        pass\n","                        #print(label, f, str(e))\n","\n","        np.random.shuffle(self.training_data)\n","        np.save(\"training_data.npy\", self.training_data)\n","        print('Cats:',dogsvcats.catcount)\n","        print('Dogs:',dogsvcats.dogcount)\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__() # just run the init of parent class (nn.Module)\n","        self.conv1 = nn.Conv2d(1, 32, 5) # input is 1 image, 32 output channels, 5x5 kernel / window\n","        self.conv2 = nn.Conv2d(32, 64, 5) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n","        self.conv3 = nn.Conv2d(64, 128, 5)\n","\n","        x = torch.randn(50,50).view(-1,1,50,50)\n","        self._to_linear = None\n","        self.convs(x)\n","\n","        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n","        self.fc2 = nn.Linear(512, 2) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n","\n","    def convs(self, x):\n","        # max pooling over 2x2\n","        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n","        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n","        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n","\n","        if self._to_linear is None:\n","            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n","        return x\n","\n","    def forward(self, x):\n","        x = self.convs(x)\n","        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x) # bc this is our output layer. No activation here.\n","        return F.softmax(x, dim=1)\n","\n","\n","net = Net()\n","print(net)\n","\n","if REBUILD_DATA:\n","    dogsvcats = DogsVSCats()\n","    dogsvcats.make_training_data()\n","\n","training_data = np.load(\"drive/My Drive/Colab Notebooks/training_data.npy\", allow_pickle=True)\n","print(len(training_data))\n","\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","loss_function = nn.MSELoss()\n","\n","X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n","X = X/255.0\n","y = torch.Tensor([i[1] for i in training_data])\n","\n","VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n","val_size = int(len(X)*VAL_PCT)\n","\n","train_X = X[:-val_size]\n","train_y = y[:-val_size]\n","\n","test_X = X[-val_size:]\n","test_y = y[-val_size:]\n","\n","BATCH_SIZE = 100\n","EPOCHS = 1\n","\n","\n","def train(net):\n","    for epoch in range(EPOCHS):\n","        for i in tqdm(range(0, len(train_X), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n","            #print(f\"{i}:{i+BATCH_SIZE}\")\n","            batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n","            batch_y = train_y[i:i+BATCH_SIZE]\n","\n","            net.zero_grad()\n","\n","            outputs = net(batch_X)\n","            loss = loss_function(outputs, batch_y)\n","            loss.backward()\n","            optimizer.step()    # Does the update\n","\n","        print(f\"Epoch: {epoch}. Loss: {loss}\")\n","\n","\n","def test(net):\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for i in tqdm(range(len(test_X))):\n","            real_class = torch.argmax(test_y[i])\n","            net_out = net(test_X[i].view(-1, 1, 50, 50))[0]  # returns a list, \n","            predicted_class = torch.argmax(net_out)\n","\n","            if predicted_class == real_class:\n","                correct += 1\n","            total += 1\n","\n","    print(\"Accuracy: \", round(correct/total, 3))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n","  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=512, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",")\n","24946\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oadOE4VDz-N-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c430d60f-865a-47ad-9aad-50bbe685fb90","executionInfo":{"status":"ok","timestamp":1584370020466,"user_tz":-420,"elapsed":885,"user":{"displayName":"Nonthakon Jitchiranant","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQRf1pDpuAGCoa63WoiIEb1FlqGrP4SE6snYtKLA=s64","userId":"05164243114410212485"}}},"source":["torch.cuda.is_available()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"JpPR_GDE0BTm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4cb03cfa-8a15-4674-ca82-a4b8cbbac2b4","executionInfo":{"status":"ok","timestamp":1584370033122,"user_tz":-420,"elapsed":938,"user":{"displayName":"Nonthakon Jitchiranant","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQRf1pDpuAGCoa63WoiIEb1FlqGrP4SE6snYtKLA=s64","userId":"05164243114410212485"}}},"source":["device = torch.device(\"cuda:0\")\n","device"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"Gxi3MjpF0BWR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e07348fc-5672-45fb-bdda-93c77e928212","executionInfo":{"status":"ok","timestamp":1584370055784,"user_tz":-420,"elapsed":858,"user":{"displayName":"Nonthakon Jitchiranant","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQRf1pDpuAGCoa63WoiIEb1FlqGrP4SE6snYtKLA=s64","userId":"05164243114410212485"}}},"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n","    print(\"Running on the GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on the CPU\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Running on the GPU\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wHaxnsQq0MZJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"f254558c-6fd3-4d9d-bab6-ed3ca8ddfd9e","executionInfo":{"status":"ok","timestamp":1584370297920,"user_tz":-420,"elapsed":23163,"user":{"displayName":"Nonthakon Jitchiranant","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQRf1pDpuAGCoa63WoiIEb1FlqGrP4SE6snYtKLA=s64","userId":"05164243114410212485"}}},"source":["EPOCHS = 3\n","\n","net = Net().to(device)\n","\n","def train(net):\n","    optimizer = optim.Adam(net.parameters(), lr=0.001)\n","    BATCH_SIZE = 100\n","    EPOCHS = 10\n","    for epoch in range(EPOCHS):\n","        for i in range(0, len(train_X), BATCH_SIZE): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n","            #print(f\"{i}:{i+BATCH_SIZE}\")\n","            batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n","            batch_y = train_y[i:i+BATCH_SIZE]\n","\n","            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","            net.zero_grad()\n","\n","            optimizer.zero_grad()   # zero the gradient buffers\n","            outputs = net(batch_X)\n","            loss = loss_function(outputs, batch_y)\n","            loss.backward()\n","            optimizer.step()    # Does the update\n","\n","        print(f\"Epoch: {epoch}. Loss: {loss}\")\n","\n","train(net)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Epoch: 0. Loss: 0.25980934500694275\n","Epoch: 1. Loss: 0.22099004685878754\n","Epoch: 2. Loss: 0.18806138634681702\n","Epoch: 3. Loss: 0.18021899461746216\n","Epoch: 4. Loss: 0.17469286918640137\n","Epoch: 5. Loss: 0.16593487560749054\n","Epoch: 6. Loss: 0.14535151422023773\n","Epoch: 7. Loss: 0.13336017727851868\n","Epoch: 8. Loss: 0.14623339474201202\n","Epoch: 9. Loss: 0.1350339651107788\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fPPMBqVI1FFR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"83b22ff7-85e3-4276-d652-267986d3ba7a","executionInfo":{"status":"ok","timestamp":1584370326767,"user_tz":-420,"elapsed":2382,"user":{"displayName":"Nonthakon Jitchiranant","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQRf1pDpuAGCoa63WoiIEb1FlqGrP4SE6snYtKLA=s64","userId":"05164243114410212485"}}},"source":["test_X.to(device)\n","test_y.to(device)\n","\n","def test(net):\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for i in tqdm(range(len(test_X))):\n","            real_class = torch.argmax(test_y[i]).to(device)\n","            net_out = net(test_X[i].view(-1, 1, 50, 50).to(device))[0]  # returns a list, \n","            predicted_class = torch.argmax(net_out)\n","\n","            if predicted_class == real_class:\n","                correct += 1\n","            total += 1\n","\n","    print(\"Accuracy: \", round(correct/total, 3))\n","\n","test(net)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["100%|██████████| 2494/2494 [00:01<00:00, 1560.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy:  0.756\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}